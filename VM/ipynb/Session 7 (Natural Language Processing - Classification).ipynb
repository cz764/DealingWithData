{
 "metadata": {
  "name": "",
  "signature": "sha256:30c42725e8574fcc521da73eedecb42d2376867da2eb5d8b9d2805c608b48d7c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Learning to Classify Text: Supervised Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For this lesson, we will focus on how to build our first automatic classification algorithms. Since the topic is huge, we will be simply scratching the surface, to get something working. For those interested in learning more, taking the Data Mining course next semester is the natural sequence."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Classification is the task of choosing the correct class label for a given input. In basic classification tasks, each input is considered in isolation from all other inputs, and the set of labels is defined in advance. Some examples of classification tasks are:\n",
      "\n",
      "* Deciding whether an email is spam or not.\n",
      "* Deciding what the topic of a news article is, from a fixed list of topic areas such as \"sports,\" \"technology,\" and \"politics.\"\n",
      "* Deciding whether a given occurrence of the word bank is used to refer to a river bank, a financial institution, the act of tilting to the side, or the act of depositing something in a financial institution.\n",
      "\n",
      "A classifier is called supervised if it is built based on **training data** containing the correct label for each input. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import Image\n",
      "Image(url=\"http://www.nltk.org/images/supervised-classification.png\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<img src=\"http://www.nltk.org/images/supervised-classification.png\"/>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 58,
       "text": [
        "<IPython.core.display.Image at 0x7f82d31a5e50>"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(a) During training, we have a set of input cases, for which we know their correct label. Then we take each input and we extract a set of _features_, which capture the basic information about each input. Pairs of feature sets and labels are fed into the machine learning algorithm to generate a model. \n",
      "\n",
      "(b) During prediction, we need to classify input for which we do not have the correct label. For that, we extract the  same set of features from the input. we feed these features into the model, which generates predicted labels.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Gender Identification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Earlier, we have seen how we can generate frequency distribution (`FreqDist`) objects from texts (or collection of texts), and we discussed how such information can be used for identification of important words in a text.\n",
      "\n",
      "Let's see how we can use these frequency distributions for our first task: Identify the gender of a name.\n",
      "\n",
      "One more wordlist corpus is the Names corpus, containing 8,000 first names categorized by gender. The male and female names are stored in separate files. Let's find names which appear in both files, i.e. names that are ambiguous for gender:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "\n",
      "names = nltk.corpus.names\n",
      "names.fileids()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "male_names = names.words('male.txt')\n",
      "print len(male_names)\n",
      "print(male_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "female_names = names.words('female.txt')\n",
      "print len(female_names)\n",
      "print female_names"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, we need to create our training data. For that, we will create a set of tuples, with the *label* for the name and the actual name:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = []\n",
      "data += [(\"female\", name) for name in female_names] \n",
      "data += [(\"male\", name) for name in male_names]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, we can build our first rudimentary classifier: We lookup a name in the list, and return the gender in the label.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def classify_name(input_name):\n",
      "    for (label, name) in data:\n",
      "        if name == input_name:\n",
      "            print label;"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's try now our classifier for a few different inputs:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "input_name = \"John\"\n",
      "print \"Trying \", input_name\n",
      "classify_name(input_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "input_name = \"Jane\"\n",
      "print \"Trying \", input_name\n",
      "classify_name(input_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "input_name = \"Leslie\"\n",
      "print \"Trying \", input_name\n",
      "classify_name(input_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "input_name = \"Panos\"\n",
      "print \"Trying \", input_name\n",
      "classify_name(input_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Apparently, our classifier has a few problems. Cannot handle at all names that are not in the training data, and has problems when the names appear in both lists. Let's see how many such names there are:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ambiguous = [w for w in male_names if w in female_names]\n",
      "print len(ambiguous)\n",
      "print ambiguous"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One way to improve our classifier is to use a bigger dataset, or count the actual frequency of each name in female and male versions, instead of having just a list. However, none of these solve the underlying problem that the classifier cannot extend beyond the training data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Featurization\n",
      "\n",
      "Featurization is a process in which we represent an input using a set of values, that are derived from the input. \n",
      "\n",
      "For example, for gender identification, the last character of the name can give hints about the gender. For example, it is well known that names ending in the letter `a` are almost always female. \n",
      "\n",
      "Let's create a revised data set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "last_char_data = [(label, name[-1]) for (label, name) in data]\n",
      "last_char_data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, we can use the concept of **conditional** frequency distribution, to compare the frequencies of each feature in the two classes:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cfd = nltk.ConditionalFreqDist(last_char_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "# Make the graphs a bit prettier, and bigger\n",
      "pd.set_option('display.mpl_style', 'default')\n",
      "plt.rcParams['figure.figsize'] = (15, 5)\n",
      "\n",
      "cfd.plot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This plot shows the number of female and male names ending with each letter of the alphabet; most names ending with a, e or i are female; names ending in h and l are ambiguous and can both male and female; names ending in k, o, r, s, and t are more likely to be male."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's build a classifier to model these differences more precisely.\n",
      "\n",
      "The first step in creating a classifier is deciding what features of the input are relevant, and how to encode those features. For this example, we'll start by just looking at the final letter of a given name. The following feature extractor function builds a dictionary containing relevant information about a given name:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gender_features(word):\n",
      "     return {'last_letter': word[-1]}\n",
      "\n",
      "gender_features('Shrek')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Of course, we can add more features if we want. (But beware, as this is not always better, as we will see later.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gender_features(word):\n",
      "     return {\n",
      "        'last_letter': word[-1],\n",
      "        'penultimate_letter': word[-2],\n",
      "        'last_two_letters': word[-2:]\n",
      "    }\n",
      "\n",
      "gender_features('Shrek')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The returned dictionary, known as a feature set, maps from features' names to their values. Feature names are case-sensitive strings that typically provide a short human-readable description of the feature. Feature values are values with simple types, such as booleans, numbers, and strings.\n",
      "\n",
      "Now that we've defined a feature extractor, we need to prepare a list of examples and corresponding class labels."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import names\n",
      "\n",
      "def gender_features(word):\n",
      "     return {'last_letter': word[-1]}\n",
      "\n",
      "male_names = names.words('male.txt')\n",
      "female_names = names.words('female.txt')\n",
      "\n",
      "labeled_names = []\n",
      "labeled_names += [(\"female\", name) for name in female_names] \n",
      "labeled_names += [(\"male\", name) for name in male_names]\n",
      "\n",
      "labeled_featuresets = [(gender_features(name), gender) for (gender, name) in labeled_names]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we divide the resulting list of feature sets into a **training set** and a **test set**. The training set is used to train our classifier. The test set will **not** be used for training but only for evaluating the performance of our classifier for \"unseen\" data that have not been present in the training data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We want to randomize the order, before separating into training and test set\n",
      "import random\n",
      "random.shuffle(labeled_featuresets)\n",
      "\n",
      "# We will keep 500 examples for testing and the remaining ones will be training\n",
      "train_set, test_set = labeled_featuresets[500:], labeled_featuresets[:500]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have our data ready, let's build our classifier. We will use a \"Naive Bayes\" classifier. We are not going to talk about the underlying mathematical details of the classification model, and instead will treat as a black box. Covering how the NB classifier works, its strengths and weaknesses, and learning about alternative classification models (e.g., decision trees, logistic regression, support vector machines, etc) is the topic of the Data Mining class."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier = nltk.NaiveBayesClassifier.train(train_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's just test it out on some names that did not appear in its training data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier.classify(gender_features('Neo'))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier.classify(gender_features('Trinity'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Observe that these character names from The Matrix are correctly classified. Although this science fiction movie is set in 2199, it still conforms with our expectations about names and genders. \n",
      "\n",
      "Let's check a few more:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "smalltest = {\"Simona\", \"Adam\", \"Sidney\", \"Phillip\", \"Won Jun\", \"Hannah\", \"Panos\", \"Marios\", \"Jonathan\", \"Zhangshuai\", \n",
      " \"John\", \"Sherif\", \"Junghoon\", \"Zhongling\", \"Raffi\", \"Casey\", \"Alexander\" }\n",
      "\n",
      "for name in smalltest:\n",
      "    features = gender_features(name)\n",
      "    print \"Name: \", name, \" ==> \", classifier.classify(features)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can systematically evaluate the classifier on a much larger quantity of unseen data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nltk.classify.accuracy(classifier, test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, we can examine the classifier to determine which features it found most effective for distinguishing the names' genders:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier.show_most_informative_features(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Exercise \n",
      "\n",
      "Modify the gender_features() function to provide the classifier with features encoding the length of the name, its first letter, and any other features that seem like they might be informative. Retrain the classifier with these new features, and test its accuracy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**\"Big\" data note**: When working with large data sets, constructing a single list that contains the features of _every_ instance can use up a large amount of memory. In these cases, use the function `nltk.classify.apply_features`, which returns an object that acts like a list but does not store all the feature sets in memory:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.classify import apply_features\n",
      "random.shuffle(labeled_names)\n",
      "train_set = apply_features(gender_features, labeled_names[500:])\n",
      "test_set = apply_features(gender_features, labeled_names[:500])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Choosing the right features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Selecting relevant features and deciding how to encode them for a learning method can have an enormous impact on the learning method's ability to extract a good model. Much of the interesting work in building a classifier is deciding what features might be relevant, and how we can represent them. Although it's often possible to get decent performance by using a fairly simple and obvious set of features, there are usually significant gains to be had by using carefully constructed features based on a thorough understanding of the task at hand.\n",
      "\n",
      "Typically, feature extractors are built through a process of trial-and-error, guided by intuitions about what information is relevant to the problem. It's common to start with a \"kitchen sink\" approach, including all the features that you can think of, and then checking to see which features actually are helpful."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gender_features_expanded(name):\n",
      "    features = {}\n",
      "    features[\"first_letter\"] = name[0].lower()\n",
      "    features[\"last_letter\"] = name[-1].lower()\n",
      "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
      "        features[\"count(%s)\" % letter] = name.lower().count(letter)\n",
      "        features[\"has(%s)\" % letter] = (letter in name.lower())\n",
      "    return features\n",
      "\n",
      "print gender_features_expanded('Panos')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "However, there are usually limits to the number of features that you should use with a given learning algorithm \u2014 if we provide too many features, then the algorithm will have a higher chance of relying on idiosyncrasies of your training data that don't generalize well to new examples. This problem is known as **overfitting**, and can be especially problematic when working with small training sets. \n",
      "\n",
      "For example, if we train a naive Bayes classifier using the feature extractor shown in 1.2, it will overfit relatively small training set, resulting in a system whose accuracy is lower than the accuracy of a classifier that only pays attention to the final letter of each name:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "random.shuffle(labeled_names)\n",
      "featuresets = [(gender_features_expanded(n), gender) for (gender, n) in labeled_names]\n",
      "train_set, test_set = featuresets[500:], featuresets[:500]\n",
      "train_names, test_names = labeled_names[500:], labeled_names[:500]\n",
      "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      "print(nltk.classify.accuracy(classifier, test_set))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, let's keep our original classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "random.shuffle(labeled_names)\n",
      "featuresets = [(gender_features(n), gender) for (gender, n) in labeled_names]\n",
      "train_set, test_set = featuresets[500:], featuresets[:500]\n",
      "train_names, test_names = labeled_names[500:], labeled_names[:500]\n",
      "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      "print(nltk.classify.accuracy(classifier, test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can generate a list of the errors that the classifier makes when predicting name genders:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "errors = []\n",
      "for (correct, name) in test_names:\n",
      "    guess = classifier.classify(gender_features(name))\n",
      "    if correct != guess:\n",
      "        errors.append( (correct, guess, name) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(errors)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can then examine individual error cases where the model predicted the wrong label, and try to determine what additional pieces of information would allow it to make the right decision (or which existing pieces of information are tricking it into making the wrong decision). The feature set can then be adjusted accordingly. The names classifier that we have built generates about 100 errors on the test corpus:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for (correct, guess, name) in sorted(errors):\n",
      "    print('correct=%-8s guess=%-8s name=%-30s' % (correct, guess, name))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Looking through this list of errors makes it clear that some suffixes that are more than one letter can be indicative of name genders. For example, names ending in yn appear to be predominantly female, despite the fact that names ending in n tend to be male; and names ending in ch are usually male, even though names that end in h tend to be female. We therefore adjust our feature extractor to include features for two-letter suffixes:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gender_features(word):\n",
      "    return {'last_letter': word[-1:],\n",
      "           'last_two_letters': word[-2:]}\n",
      "\n",
      "random.shuffle(labeled_names)\n",
      "featuresets = [(gender_features(n), gender) for (gender, n) in labeled_names]\n",
      "train_set, test_set = featuresets[500:], featuresets[:500]\n",
      "train_names, test_names = labeled_names[500:], labeled_names[:500]\n",
      "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      "print(nltk.classify.accuracy(classifier, test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This error analysis procedure can then be repeated, checking for patterns in the errors that are made by the newly improved classifier. Each time the error analysis procedure is repeated, we should select a different test/training split, to ensure that the classifier does not start to reflect idiosyncrasies in the test set."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Document Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A common classification task is to classify documents into categories. Let's use for this the Movie Reviews corpus from NLTK:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import movie_reviews\n",
      "\n",
      "categories = movie_reviews.categories()\n",
      "categories"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's generate the list of files, each with its corresponding category."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "labeled_files = []\n",
      "for category in categories:\n",
      "    labeled_files += [(fileid, category) for fileid in movie_reviews.fileids(category)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "labeled_files[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "labeled_files[-5:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(labeled_files)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len([l for l in labeled_files if l[1]=='pos'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len([l for l in labeled_files if l[1]=='neg'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's create the features. We will create one feature per word, with a binary value, indicating whether the document contains the word or not. To limit the number of features that the classifier needs to process, we begin by constructing a list of the 2000 most frequent words in the overall corpus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "\n",
      "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words() if w.isalpha())\n",
      "word_features = all_words.keys()[:2000]\n",
      "\n",
      "def document_features(fileid):\n",
      "    document_words = set(movie_reviews.words(fileid))\n",
      "    features = {}\n",
      "    for word in word_features:\n",
      "        features['contains(%s)' % word] = (word.lower() in document_words)\n",
      "    return features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit  document_features(\"pos/cv995_21821.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can see from the result above that the extraction of features from each dcouments can take 10-30 msecs each. So for our dataset of 2000 documents, we can expect a feature extraction time of 20-60 seconds. We will use the \"apply_features\" trick (see above) to avoid doing all the work before actually having to train the classifier."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.classify import apply_features\n",
      "random.shuffle(labeled_files)\n",
      "train_set = apply_features(document_features, labeled_files[100:])\n",
      "test_set = apply_features(document_features, labeled_files[:100])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Alternatively, to avoid using the \"apply_features\"\n",
      "# labeled_documents = [(document_features(fileid), category) for (fileid, category) in labeled_files]\n",
      "# train_set, test_set = labeled_documents[100:], labeled_documents[:100]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier = nltk.NaiveBayesClassifier.train(train_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nltk.classify.accuracy(classifier, test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier.show_most_informative_features(20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise\n",
      "\n",
      "Try to come up with features that will improve the classifier that we discussed above. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Modify the code below\n",
      "\n",
      "import nltk\n",
      "from nltk.classify import apply_features\n",
      "\n",
      "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words() if w.isalpha())\n",
      "word_features = all_words.keys()[:2000]\n",
      "\n",
      "def document_features(fileid):\n",
      "    # Modify to add your own set of features\n",
      "    document_words = set(movie_reviews.words(fileid))\n",
      "    features = {}\n",
      "    for word in word_features:\n",
      "        features['contains(%s)' % word] = (word.lower() in document_words)\n",
      "    return features\n",
      "\n",
      "\n",
      "random.shuffle(labeled_files)\n",
      "train_set = apply_features(document_features, labeled_files[200:])\n",
      "test_set = apply_features(document_features, labeled_files[:200])\n",
      "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      "print \"Accuracy:\", nltk.classify.accuracy(classifier, test_set))\n",
      "classifier.show_most_informative_features(20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Further example: Sentence Segmentation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sentence segmentation can be viewed as a classification task for punctuation: whenever we encounter a symbol that could possibly end a sentence, such as a period or a question mark, we have to decide whether it terminates the preceding sentence.\n",
      "\n",
      "The first step is to obtain some data that has already been segmented into sentences and convert it into a form that is suitable for extracting features:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sents = nltk.corpus.treebank_raw.sents()\n",
      "tokens = []\n",
      "boundaries = set()\n",
      "offset = 0\n",
      "for sent in sents:\n",
      "    tokens.extend(sent)\n",
      "    offset += len(sent)\n",
      "    boundaries.add(offset-1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, tokens is a merged list of tokens from the individual sentences, and boundaries is a set containing the indexes of all sentence-boundary tokens. Next, we need to specify the features of the data that will be used in order to decide whether punctuation indicates a sentence-boundary:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def punct_features(tokens, i):\n",
      "    return {'next-word-capitalized': tokens[i+1][0].isupper(),\n",
      "        'prev-word': tokens[i-1].lower(),\n",
      "        'punct': tokens[i],\n",
      "        'prev-word-is-one-char': len(tokens[i-1]) == 1}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Based on this feature extractor, we can create a list of labeled featuresets by selecting all the punctuation tokens, and tagging whether they are boundary tokens or not:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "featuresets = [(punct_features(tokens, i), (i in boundaries))\n",
      "                for i in range(1, len(tokens)-1)\n",
      "                if tokens[i] in '.?!']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using these featuresets, we can train and evaluate a punctuation classifier:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "size = int(len(featuresets) * 0.1)\n",
      "train_set, test_set = featuresets[size:], featuresets[:size]\n",
      "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      "nltk.classify.accuracy(classifier, test_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To use this classifier to perform sentence segmentation, we simply check each punctuation mark to see whether it's labeled as a boundary; and divide the list of words at the boundary marks. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def segment_sentences(words):\n",
      "    start = 0\n",
      "    sents = []\n",
      "    for i, word in enumerate(words):\n",
      "        if word in '.?!' and classifier.classify(punct_features(words, i)) == True:\n",
      "            sents.append(words[start:i+1])\n",
      "            start = i+1\n",
      "    if start < len(words):\n",
      "        sents.append(words[start:])\n",
      "    return sents"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}