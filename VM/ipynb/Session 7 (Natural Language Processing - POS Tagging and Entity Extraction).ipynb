{
 "metadata": {
  "name": "",
  "signature": "sha256:c167f8e007e7b07e315f10fb953533ca67e5926b1d52e3275d99a1ea38ce3694"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Categorizing and Tagging Words"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Back in elementary school you learnt the difference between nouns, verbs, adjectives, and adverbs. These are  very useful categories for many language processing tasks. Our goals chapter is to answer the following questions:\n",
      "\n",
      "1. What are lexical categories and how are they used in natural language processing?\n",
      "2. What is a good Python data structure for storing words and their categories?\n",
      "3. How can we automatically tag each word of a text with its word class?\n",
      "\n",
      "The process of classifying words into their parts of speech and labeling them accordingly is known as part-of-speech tagging, POS-tagging, or simply tagging. \n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Using a POS tagger"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A part-of-speech tagger, or POS-tagger, processes a sequence of words, and attaches a part of speech tag to each word:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "\n",
      "text = nltk.word_tokenize(\"And now for something completely different\")\n",
      "\n",
      "nltk.pos_tag(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "[('And', 'CC'),\n",
        " ('now', 'RB'),\n",
        " ('for', 'IN'),\n",
        " ('something', 'NN'),\n",
        " ('completely', 'RB'),\n",
        " ('different', 'JJ')]"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we see that and is CC, a coordinating conjunction; now and completely are RB, or adverbs; for is IN, a preposition; something is NN, a noun; and different is JJ, an adjective.\n",
      "\n",
      "NLTK provides documentation for each tag, which can be queried using the tag, e.g. `nltk.help.upenn_tagset('RB')`, or a regular expression, e.g. `nltk.help.upenn_tagset('NN.*')`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.help.upenn_tagset('RB')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "RB: adverb\n",
        "    occasionally unabatingly maddeningly adventurously professedly\n",
        "    stirringly prominently technologically magisterially predominately\n",
        "    swiftly fiscally pitilessly ...\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.help.upenn_tagset('JJ.*')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "JJ: adjective or numeral, ordinal\n",
        "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
        "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
        "    multilingual multi-disciplinary ...\n",
        "JJR: adjective, comparative\n",
        "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
        "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
        "    cozier creamier crunchier cuter ...\n",
        "JJS: adjective, superlative\n",
        "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
        "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
        "    dearest deepest densest dinkiest ...\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's look at another example, this time including some **homonyms**:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = nltk.word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
      "nltk.pos_tag(text)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "[('They', 'PRP'),\n",
        " ('refuse', 'VBP'),\n",
        " ('to', 'TO'),\n",
        " ('permit', 'VB'),\n",
        " ('us', 'PRP'),\n",
        " ('to', 'TO'),\n",
        " ('obtain', 'VB'),\n",
        " ('the', 'DT'),\n",
        " ('refuse', 'NN'),\n",
        " ('permit', 'NN')]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that refuse and permit both appear as a present tense verb (VBP) and a noun (NN). E.g. refUSE is a verb meaning \"deny,\" while REFuse is a noun meaning \"trash\" (i.e. they are not homophones). Thus, we need to know which word is being used in order to pronounce the text correctly. (For this reason, text-to-speech systems usually perform POS-tagging.)\n",
      "\n",
      "See now how this information can be useful when trying to figure out the sense of a word in WordNet:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import wordnet as wn\n",
      "wn.synsets('refuse')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "[Synset('garbage.n.01'),\n",
        " Synset('refuse.v.01'),\n",
        " Synset('refuse.v.02'),\n",
        " Synset('defy.v.02'),\n",
        " Synset('deny.v.04'),\n",
        " Synset('resist.v.05'),\n",
        " Synset('reject.v.06')]"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "senses = [(s.lemma_names(), s.definition(), s.examples()) for s in wn.synsets('refuse')]\n",
      "for s in senses:\n",
      "    print \"Lemma name:\", s[0]\n",
      "    print \"Definition:\", s[1]\n",
      "    print \"Examples  :\", s[2]\n",
      "    print \"=======================\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Lemma name: [u'garbage', u'refuse', u'food_waste', u'scraps']\n",
        "Definition: food that is discarded (as from a kitchen)\n",
        "Examples  : []\n",
        "=======================\n",
        "Lemma name: [u'refuse', u'decline']\n",
        "Definition: show unwillingness towards\n",
        "Examples  : [u'he declined to join the group on a hike']\n",
        "=======================\n",
        "Lemma name: [u'refuse', u'reject', u'pass_up', u'turn_down', u'decline']\n",
        "Definition: refuse to accept\n",
        "Examples  : [u'He refused my offer of hospitality']\n",
        "=======================\n",
        "Lemma name: [u'defy', u'resist', u'refuse']\n",
        "Definition: elude, especially in a baffling way\n",
        "Examples  : [u'This behavior defies explanation']\n",
        "=======================\n",
        "Lemma name: [u'deny', u'refuse']\n",
        "Definition: refuse to let have\n",
        "Examples  : [u'She denies me every pleasure', u'he denies her her weekly allowance']\n",
        "=======================\n",
        "Lemma name: [u'resist', u'reject', u'refuse']\n",
        "Definition: resist immunologically the introduction of some foreign tissue or organ\n",
        "Examples  : [u'His body rejected the liver of the donor']\n",
        "=======================\n",
        "Lemma name: [u'reject', u'turn_down', u'turn_away', u'refuse']\n",
        "Definition: refuse entrance or membership\n",
        "Examples  : [u'They turned away hundreds of fans', u'Black people were often rejected by country clubs']\n",
        "=======================\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There is just one interpretation of _refuse_ that is a noun (garbage.n.01) and the most common interpretation of refuse as a verb means \"show unwillingness towards\" which is the correct interpretation in our context. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Exercise\n",
      "\n",
      "Many words, like ski and race, can be used as nouns or verbs with no difference in pronunciation. Can you think of others? Now make up a sentence with both uses of this word, and run the POS-tagger on this sentence."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Representing Tagged Tokens"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By convention in NLTK, a tagged token is represented using a **tuple** consisting of the token and the tag. We can create one of these special tuples from the standard string representation of a tagged token, using the function str2tuple():"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = nltk.word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
      "tagged = nltk.pos_tag(text)\n",
      "tagged_token = tagged[0]\n",
      "\n",
      "print tagged_token\n",
      "print tagged_token[0]\n",
      "print tagged_token[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('They', 'PRP')\n",
        "They\n",
        "PRP\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print text\n",
      "tokens = [a for (a, b) in tagged]\n",
      "print tokens\n",
      "tags = [b for (a, b) in tagged]\n",
      "print tags\n",
      "fd = nltk.FreqDist(tags)\n",
      "print fd\n",
      "fd.tabulate()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['They', 'refuse', 'to', 'permit', 'us', 'to', 'obtain', 'the', 'refuse', 'permit']\n",
        "['They', 'refuse', 'to', 'permit', 'us', 'to', 'obtain', 'the', 'refuse', 'permit']\n",
        "['PRP', 'VBP', 'TO', 'VB', 'PRP', 'TO', 'VB', 'DT', 'NN', 'NN']\n",
        "<FreqDist with 6 samples and 10 outcomes>\n",
        "  VB   NN   TO  PRP  VBP   DT \n",
        "   2    2    2    2    1    1 \n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Exercise \n",
      "\n",
      "Load a text of your choice, tokenize it, and perform part of speech tagging on it. Then extract the nouns from the text, and perform a frequency anaysis, to identify the most common nouns in the text. (Warning: POS tagging takes a good amount of time when processing long texts, so try to select a text with less than 10K tokens, or simply perform POS tagging on the first 10K-20K tokens).\n",
      "\n",
      "Repeat the exercise for adjectives.\n",
      "\n",
      "PS: If you want to parse text from HTML without resorting to XPath expressions, you can use the \"BeautifulSoup\" library as follows:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "\n",
      "url = \"http://www.nytimes.com/2014/11/11/world/asia/obama-apec-china-hong-kong.html\"\n",
      "resp = requests.get(url)\n",
      "html = resp.text \n",
      "raw = BeautifulSoup(html).get_text()\n",
      "\n",
      "# The code below is to remove the junk that was extracted in addition to the article\n",
      "start = raw.index(u\"BEIJING  \u2014\")\n",
      "end = raw.index(u\"close story-body \")\n",
      "raw = raw[start:end]\n",
      "\n",
      "# Let's do the NLTK stuff\n",
      "tokens = nltk.word_tokenize(raw)\n",
      "tagged = nltk.pos_tag(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Primitive sentiment analysis\n",
      "\n",
      "Adjectives are known to be the primary carriers of sentiment. So now let's pick a piece of text and identify the adjectives that appear in the text and their sentiment score. For that, we will use the  SentiWordNet, a lexical resource for opinion mining."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# See http://www.nltk.org/_modules/nltk/corpus/reader/sentiwordnet.html for the documentation\n",
      "\n",
      "from nltk.corpus import sentiwordnet as swn\n",
      "print(swn.senti_synset('breakdown.n.03'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<breakdown.n.03: PosScore=0.0 NegScore=0.25>\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's analyze a review text "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Amazon review for Samsung Galaxy S5, White 16GB\n",
      "# http://www.amazon.com/review/R3UULR1IWEUS4I/ref=cm_cr_dp_title?ie=UTF8&ASIN=B00IZ1X21K&nodeID=2335752011&store=wireless\n",
      "\n",
      "\n",
      "content = u'''\n",
      "First off, I am not a professional reviewer, nor am I employed or compensated by Samsung or any other company. Instead of boring you with facts - which you can find anywhere on the Net - I will just give you some real-world impressions on how it looks, feels, and runs. With that out of the way, let's get to the point and the nitty gritty, shall we?\n",
      "\n",
      "* THE SCREEN - that is the very first thing you will notice when you look at the S5. Samsung has found its niche with AMOLED screens, which are BRIGHT & SATURATED. Everything almost literally jumps out at you, and sometimes even too much so. I had to switch to the \"natural\" setting, as the \"vivid\" and even \"standard\" profiles are too saturated(and FAKE) for me. It's better as a demo unit to draw you in, but for everyday use, I recommend switching to the natural profile.\n",
      "FACTS: The Galaxy S5 has a 5.1-inch Super AMOLED capacitive touchscreen with Full HD resolution - 1080 x 1920 pixels or ~432 ppi pixel density, plus Gorilla Glass 3 to protect the screen from scratches.\n",
      "\n",
      "* The Look - the S5 has a more squared-off edges look than the S4, which is more squared off than the S3, but all three are not as angular as the S2. In terms of roundness-to square-ness, it goes from the S3 - S4 - S5 - S2 (the original S just looks like an iPhone 3GS). Check out my images for an easier comparison. The S5 is the tallest and widest, but not the thickest of the Galaxy S's. The best thing I can say about this is it's an evolution. Beauty is subjective, so judge for yourself. The front side is almost the same as any other Galaxy phone: You have the physical Home button, flanked by the \"back\" and \"menu\" capacitive buttons. Probably the most improved aspect of the design is in its functionality - it is now dust-proof, and water-proof up to 3 feet!\n",
      "FACTS: The dimensions are 5.59\" x 2.85\" x 0.32\"(142cm x 72.5cm x 8.1cm), and weighs 5.11oz(145g).\n",
      "\n",
      "* The Feel - Samsung has taken a lot of flack for making the Galaxy S line so cheap looking and feeling with its plastic bodies, for being the top Android phone maker. HTC has been known to have the best craftsmanship with their all-metal One phones. Perhaps Samsung feel they are so dominant that they don't have to spend more to mass-produce metal phones, but since they don't want to come off as too arrogant, so their compromise is a dimpled, faux-rubber backside like the Nexus 7(2012) and its very own Galaxy Note 3. It definitely gives a better feel - it doesn't slip and slide in your hands or pockets anymore - but it cannot compare to the feel and craftsmanship of the HTC One(both the m7 and m8). It is on the right track though, so let's hope that rumored luxury \"F\" line or next year's S6 will continue to get better.\n",
      "\n",
      "* How it Runs - This phone is fast, fast, FAST! With a 2.5gHz Snapdragon 801, it has the fastest processor out there right now. It terms of real speed, I cannot say if it is faster than the HTC One m8 or the Sony Xperia Z2, but it is definitely up there. When you touch an app icon to launch it, it launches nearly instantly. To really see how this phone flies, just open the gallery app and scroll through all your photos and you'll see what I mean. Usually the gallery is where most phones stutter as it tries to load all your photos and albums - but NOT the S5!\n",
      "\n",
      "* The Camera - FINALLY! Samsung has decided to make a decent camera, and not just as an afterthought. This 16mp camera is really awesome, so much better than the S4. I would always get washed out images with my S3/S4/Note 2, but with the S5, it actually looks like it's from a decent point-and-shoot dedicated camera with crisp, bright, and saturated images. Low-light shooting is also vastly improved, although not as good as the new HTC One m8. 16mp means 5312 x 2988 -resolution images, so you can actually blow them up or crop them down without fearing the dreaded pixelation monster. There are a myriad of other cool and useful camera features that I will save for you to find out(like macro and \"Google Street View\" modes :]). And lastly, the focus is quick, quick, QUICK! Nearly instantaneous focus allows you to capture those hard-to-capture moments easier. A definitely thumbs up to Samsung for paying attention to the camera and its functions.\n",
      "\n",
      "* Software - I'm still trying to figure out everything, as there is A LOT of stuff under the hood. Samsung's TouchWiz user interface this time around is A LOT less intrusive though, as much as can be without being totally stock Android, I guess. The layout and iconography are flatter and simpler, and for the better in my view. There is also a new sensor on the back, just beneath the camera lens. It is a heart-rate monitor/pedometer, and it comes with its own health app called S Health. There is a new battery-saving mode which can save you precious minutes when you're caught in a bind. All in all, I think this version is a lot nicer-looking, more responsive, and better than the precious S phones.\n",
      "\n",
      "The ultimate question is whether this phone is a worthy upgrade over the S4. As my review title suggests, it is an evolution, an incremental upgrade over the S4. So with that said I cannot whole-heartedly recommend it if you already have a good phone, or even over the S4. But I do feel this upgrade is more vast and much better than from the S3 to the S4, so in that sense Samsung has done a much better job this year. If you are switching from an older phone that was made at least 2 years ago, then I would tell you jump right in and try the S5 - it will not disappoint you. But for those with already a good phone, and/or say you just finished year one of your 2-year contract, then I would say think hard before you make the leap. For my money, I think the Note 4 and S6 will be the bigger upgrades more worth waiting for.\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokens = nltk.word_tokenize(content)\n",
      "text = nltk.Text(tokens)\n",
      "tagged = nltk.pos_tag(tokens)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's keep the adjectives only\n",
      "adjectives = [w for (w,t) in tagged if t=='JJ']\n",
      "print adjectives \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'professional', u'other', u'real-world', u'very', u'first', u'natural', u'vivid', u'standard', u'natural', u'5.1-inch', u'capacitive', u'squared-off', u'angular', u'roundness-to', u'square-ness', u'original', u'subjective', u'same', u'other', u'physical', u'capacitive', u'dust-proof', u'water-proof', u'plastic', u'top', u'all-metal', u'dominant', u'mass-produce', u'arrogant', u'very', u'own', u'next', u'real', u'open', u'S3/S4/Note', u'bright', u'Low-light', u'good', u'new', u'dreaded', u'myriad', u'other', u'useful', u'quick', u'quick', u'hard-to-capture', u'intrusive', u'much', u'new', u'heart-rate', u'own', u'new', u'battery-saving', u'precious', u'responsive', u'precious', u'ultimate', u'worthy', u'incremental', u'good', u'feel', u'vast', u'much', u'good', u'and/or', u'2-year']\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now we want to use WordNet and eliminate the words that do not appear in our lexicon\n",
      "# Since we do not have much of information for further disambiguation, we will keep only the \n",
      "# most popular interpretation (list element 0) if there are multiple ones\n",
      "resolved_adjectives = [(w, swn.senti_synsets(w, 'a')[0]) for w in adjectives if len(swn.senti_synsets(w, 'a'))>0]\n",
      "print resolved_adjectives"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(u'professional', SentiSynset('professional.a.01')), (u'other', SentiSynset('other.a.01')), (u'first', SentiSynset('first.a.01')), (u'natural', SentiSynset('natural.a.01')), (u'standard', SentiSynset('standard.a.01')), (u'natural', SentiSynset('natural.a.01')), (u'capacitive', SentiSynset('capacitive.a.01')), (u'angular', SentiSynset('angular.a.01')), (u'original', SentiSynset('original.a.03')), (u'subjective', SentiSynset('subjective.a.01')), (u'same', SentiSynset('same.a.01')), (u'other', SentiSynset('other.a.01')), (u'physical', SentiSynset('physical.a.01')), (u'capacitive', SentiSynset('capacitive.a.01')), (u'top', SentiSynset('top.a.01')), (u'dominant', SentiSynset('dominant.a.01')), (u'real', SentiSynset('real.a.01')), (u'open', SentiSynset('open.a.01')), (u'bright', SentiSynset('bright.a.01')), (u'good', SentiSynset('good.a.01')), (u'new', SentiSynset('new.a.01')), (u'other', SentiSynset('other.a.01')), (u'useful', SentiSynset('useful.a.01')), (u'intrusive', SentiSynset('intrusive.a.01')), (u'much', SentiSynset('much.a.01')), (u'new', SentiSynset('new.a.01')), (u'new', SentiSynset('new.a.01')), (u'responsive', SentiSynset('responsive.a.01')), (u'ultimate', SentiSynset('ultimate.a.01')), (u'worthy', SentiSynset('worthy.a.01')), (u'good', SentiSynset('good.a.01')), (u'much', SentiSynset('much.a.01')), (u'good', SentiSynset('good.a.01'))]\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# SentiWordNet assigns to each synset of WordNet three\n",
      "# sentiment scores: positivity, negativity, and objectivity.\n",
      "\n",
      "for (w,a) in resolved_adjectives:\n",
      "    print \"Word:\", w\n",
      "    print \"Synset:\", a\n",
      "    print \"Pos score:\",  a.pos_score()\n",
      "    print \"Neg score:\",  a.neg_score()\n",
      "    print \"Objectivity score:\",  a.obj_score()\n",
      "    print \"======================================\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Word: professional\n",
        "Synset: <professional.a.01: PosScore=0.0 NegScore=0.0>\n",
        "Pos score: 0.0\n",
        "Neg score: 0.0\n",
        "Objectivity score: 1.0\n",
        "======================================\n",
        "Word: other\n",
        "Synset: <other.a.01: PosScore=0.0 NegScore=0.625>\n",
        "Pos score: 0.0\n",
        "Neg score: 0.625\n",
        "Objectivity score: 0.375\n",
        "======================================\n",
        "Word: first\n",
        "Synset: <first.a.01: PosScore=0.0 NegScore=0.0>\n",
        "Pos score: 0.0\n",
        "Neg score: 0.0\n",
        "Objectivity score: 1.0\n",
        "======================================\n",
        "Word: natural\n",
        "Synset: <natural.a.01: PosScore=0.25 NegScore=0.0>\n",
        "Pos score: 0.25\n",
        "Neg score: 0.0\n",
        "Objectivity score: 0.75\n",
        "======================================\n",
        "Word: standard\n",
        "Synset: <standard.a.01: PosScore=0.375 NegScore=0.375>\n",
        "Pos score: 0.375\n",
        "Neg score: 0.375\n",
        "Objectivity score: 0.25\n",
        "======================================\n",
        "Word: natural\n",
        "Synset: <natural.a.01: PosScore=0.25 NegScore=0.0>\n",
        "Pos score: 0.25\n",
        "Neg score: 0.0\n",
        "Objectivity score: 0.75\n",
        "======================================\n",
        "Word: capacitive\n",
        "Synset: <capacitive.a.01: PosScore=0.0 NegScore=0.0>\n",
        "Pos score: 0.0\n",
        "Neg score: 0.0\n",
        "Objectivity score: 1.0\n",
        "======================================\n",
        "Word: angular\n",
        "Synset: <angular.a.01: PosScore=0.0 NegScore=0.0>\n",
        "Pos score: 0.0\n",
        "Neg score: 0.0\n",
        "Objectivity score: 1.0\n",
        "======================================\n",
        "Word: original\n",
        "Synset: <original.a.03: PosScore=0.25 NegScore=0.0>\n",
        "Pos score: 0.25\n",
        "Neg score: 0.0\n",
        "Objectivity score: 0.75\n",
        "======================================\n",
        "Word: subjective\n",
        "Synset: <subjective.a.01: PosScore=0.0 NegScore=0.0>\n",
        "Pos score: 0.0\n",
        "Neg score: 0.0\n",
        "Objectivity score: 1.0\n",
        "======================================\n",
        "Word: same\n",
        "Synset: <same.a.01: PosScore=0.0 NegScore=0.0>\n",
        "Pos score: 0.0\n",
        "Neg score: 0.0\n",
        "Objectivity score: 1.0\n",
        "======================================\n",
        "Word: other\n",
        "Synset: <other.a.01: PosScore=0.0 NegScore=0.625>\n",
        "Pos score: 0.0\n",
        "Neg score: 0.625\n",
        "Objectivity score: 0.375\n",
        "======================================\n",
        "Word: physical\n",
        "Synset: <physical.a.01: PosScore=0.0 NegScore=0.0>\n",
        "Pos score: 0.0\n",
        "Neg score: 0.0\n",
        "Objectivity score: 1.0\n",
        "======================================\n",
        "Word: capacitive\n",
        "Synset: <capacitive.a.01: PosScore=0.0 NegScore=0.0>\n",
        "Pos score: 0.0\n",
        "Neg score: 0.0\n",
        "Objectivity score: 1.0\n",
        "======================================\n",
        "Word: top\n",
        "Synset: <top.a.01: PosScore=0.0 NegScore=0.0>\n",
        "Pos score: 0.0\n",
        "Neg score: 0.0\n",
        "Objectivity score: 1.0\n",
        "======================================\n",
        "Word: dominant\n",
        "Synset: <dominant.a.01: PosScore=0.0 NegScore=0.0>\n",
        "Pos score: 0.0\n",
        "Neg score: 0.0\n",
        "Objectivity score: 1.0\n",
        "======================================\n",
        "Word: real\n",
        "Synset: <real.a.01: PosScore=0.0 NegScore=0.0>\n",
        "Pos score: 0.0\n",
        "Neg score: 0.0\n",
        "Objectivity score: 1.0\n",
        "======================================\n",
        "Word: open\n",
        "Synset: <open.a.01: PosScore=0.0 NegScore=0.375>\n",
        "Pos score: 0.0\n",
        "Neg score: 0.375\n",
        "Objectivity score: 0.625\n",
        "======================================\n",
        "Word: bright\n",
        "Synset: <bright.a.01: PosScore=0.125 NegScore=0.0>\n",
        "Pos score: 0.125\n",
        "Neg score: 0.0\n",
        "Objectivity score: 0.875\n",
        "======================================\n",
        "Word: good\n",
        "Synset: <good.a.01: PosScore=0.75 NegScore=0.0>\n",
        "Pos score:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.75\n",
        "Neg score: 0.0\n",
        "Objectivity score: 0.25\n",
        "======================================\n",
        "Word: new\n",
        "Synset: <new.a.01: PosScore=0.375 NegScore=0.0>\n",
        "Pos score: 0.375\n",
        "Neg score: 0.0\n",
        "Objectivity score: 0.625\n",
        "======================================\n",
        "Word: other\n",
        "Synset: <other.a.01: PosScore=0.0 NegScore=0.625>\n",
        "Pos score: 0.0\n",
        "Neg score: 0.625\n",
        "Objectivity score: 0.375\n",
        "======================================\n",
        "Word: useful\n",
        "Synset: <useful.a.01: PosScore=0.0 NegScore=0.0>\n",
        "Pos score: 0.0\n",
        "Neg score: 0.0\n",
        "Objectivity score: 1.0\n",
        "======================================\n",
        "Word: intrusive\n",
        "Synset: <intrusive.a.01: PosScore=0.125 NegScore=0.5>\n",
        "Pos score: 0.125\n",
        "Neg score: 0.5\n",
        "Objectivity score: 0.375\n",
        "======================================\n",
        "Word: much\n",
        "Synset: <much.a.01: PosScore=0.0 NegScore=0.0>\n",
        "Pos score: 0.0\n",
        "Neg score: 0.0\n",
        "Objectivity score: 1.0\n",
        "======================================\n",
        "Word: new\n",
        "Synset: <new.a.01: PosScore=0.375 NegScore=0.0>\n",
        "Pos score: 0.375\n",
        "Neg score: 0.0\n",
        "Objectivity score: 0.625\n",
        "======================================\n",
        "Word: new\n",
        "Synset: <new.a.01: PosScore=0.375 NegScore=0.0>\n",
        "Pos score: 0.375\n",
        "Neg score: 0.0\n",
        "Objectivity score: 0.625\n",
        "======================================\n",
        "Word: responsive\n",
        "Synset: <responsive.a.01: PosScore=0.0 NegScore=0.0>\n",
        "Pos score: 0.0\n",
        "Neg score: 0.0\n",
        "Objectivity score: 1.0\n",
        "======================================\n",
        "Word: ultimate\n",
        "Synset: <ultimate.a.01: PosScore=0.0 NegScore=0.125>\n",
        "Pos score: 0.0\n",
        "Neg score: 0.125\n",
        "Objectivity score: 0.875\n",
        "======================================\n",
        "Word: worthy\n",
        "Synset: <worthy.a.01: PosScore=0.875 NegScore=0.0>\n",
        "Pos score: 0.875\n",
        "Neg score: 0.0\n",
        "Objectivity score: 0.125\n",
        "======================================\n",
        "Word: good\n",
        "Synset: <good.a.01: PosScore=0.75 NegScore=0.0>\n",
        "Pos score: 0.75\n",
        "Neg score: 0.0\n",
        "Objectivity score: 0.25\n",
        "======================================\n",
        "Word: much\n",
        "Synset: <much.a.01: PosScore=0.0 NegScore=0.0>\n",
        "Pos score: 0.0\n",
        "Neg score: 0.0\n",
        "Objectivity score: 1.0\n",
        "======================================\n",
        "Word: good\n",
        "Synset: <good.a.01: PosScore=0.75 NegScore=0.0>\n",
        "Pos score: 0.75\n",
        "Neg score: 0.0\n",
        "Objectivity score: 0.25\n",
        "======================================\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# But let's take a look at what we rejected\n",
      "rejected_adjectives = [w for w in adjectives if len(swn.senti_synsets(w, 'a'))==0]\n",
      "print rejected_adjectives\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'real-world', u'very', u'vivid', u'5.1-inch', u'squared-off', u'roundness-to', u'square-ness', u'dust-proof', u'water-proof', u'plastic', u'all-metal', u'mass-produce', u'arrogant', u'very', u'own', u'next', u'S3/S4/Note', u'Low-light', u'dreaded', u'myriad', u'quick', u'quick', u'hard-to-capture', u'heart-rate', u'own', u'battery-saving', u'precious', u'precious', u'incremental', u'feel', u'vast', u'and/or', u'2-year']\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Perhaps we would also like to figure out what the adjectives in the text refer to. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(0, len(tagged)):\n",
      "    current_word = tagged[i][0]\n",
      "    current_pos = tagged[i][1]\n",
      "    if current_pos == 'NN':\n",
      "        previous_word = tagged[i-1][0]\n",
      "        previous_pos = tagged[i-1][1]\n",
      "        if previous_pos == 'JJ':\n",
      "            print previous_word + \" \" + current_word"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "professional reviewer\n",
        "other company\n",
        "first thing\n",
        "natural profile\n",
        "capacitive touchscreen\n",
        "mass-produce metal\n",
        "next year\n",
        "real speed\n",
        "Low-light shooting\n",
        "dreaded pixelation\n",
        "other cool\n",
        "useful camera\n",
        "new sensor\n",
        "heart-rate monitor/pedometer\n",
        "own health\n",
        "battery-saving mode\n",
        "ultimate question\n",
        "worthy upgrade\n",
        "incremental upgrade\n",
        "good phone\n",
        "much better\n",
        "good phone\n",
        "and/or say\n",
        "2-year contract\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Excercise 1\n",
      "\n",
      "Try a new text with this type of sentiment analysis. Let's figure out what works and what does not\n",
      "\n",
      "#### Exercise 2\n",
      "\n",
      "Instead of adjectives-nouns, we can instead use adverbs and verbs (e.g., \"works nicely\"). Let's modify the code above to extract patterns involving verbs and adverbs\n",
      "\n",
      "#### Exercise 3\n",
      "\n",
      "How can you modify the code to find more patterns, instead of just JJ-NN (adjective followed by noun)?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Named Entity Recognition\n",
      "\n",
      "Named entities are definite noun phrases that refer to specific types of individuals, such as organizations, persons, dates, and so on. Here is a list of some commonly used Named Entities:\n",
      " \n",
      "* ORGANIZATION\t(e.g., Georgia-Pacific Corp., WHO)\n",
      "* PERSON\t(e.g., Eddy Bonte, President Obama)\n",
      "* LOCATION\t(e.g., Murray River, Mount Everest)\n",
      "* DATE\t(e.g., June, 2008-06-29)\n",
      "* TIME\t(e.g., two fifty a m, 1:30 p.m.)\n",
      "* MONEY\t(e.g., 175 million Canadian Dollars, GBP 10.40)\n",
      "* PERCENT\t(e.g., twenty pct, 18.75 %)\n",
      "* FACILITY\t(e.g., Washington Monument, Stonehenge)\n",
      "* GPE\t(e.g., South East Asia, Midlothian)\n",
      "\n",
      "The goal of a named entity recognition (NER) system is to identify all textual mentions of the named entities. This can be broken down into two sub-tasks: identifying the boundaries of the NE, and identifying its type.\n",
      "\n",
      "NLTK provides a module that has already been trained to recognize named entities, accessed with the function nltk.ne_chunk(). If we set the parameter `binary=True`, then named entities are just tagged as NE; otherwise, the classifier adds category labels such as PERSON, ORGANIZATION, and GPE."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw = u'''\n",
      "BEIJING \u2014 President Obama appealed on Monday for official restraint in dealing with the pro-democracy demonstrations in Hong Kong, saying that while the United States did not necessarily agree with China about the dispute, it did not want to see the tensions in the city erupt into violence.\n",
      "\n",
      "Mr. Obama\u2019s carefully calibrated remarks were his first on the Hong Kong protests since arriving in Beijing earlier in the day. \u201cObviously, the situation between China and Hong Kong is historically complicated and is in the process of transition,\u201d Mr. Obama said.\n",
      "\n",
      "\u201cOur primary message has been to make sure violence is avoided,\u201d the president said, adding, \u201cWe don\u2019t expect China to follow an American model in every instance. But we\u2019re going to continue to have concerns about human rights.\u201d\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentences = nltk.sent_tokenize(raw)\n",
      "sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
      "sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
      "\n",
      "for sent in sentences:\n",
      "    named_entities = nltk.ne_chunk(sent, binary=False)\n",
      "    print named_entities"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (GPE BEIJING/NNP)\n",
        "  \\u2014/:\n",
        "  President/NNP\n",
        "  (PERSON Obama/NNP)\n",
        "  appealed/VBD\n",
        "  on/IN\n",
        "  Monday/NNP\n",
        "  for/IN\n",
        "  official/NN\n",
        "  restraint/NN\n",
        "  in/IN\n",
        "  dealing/NN\n",
        "  with/IN\n",
        "  the/DT\n",
        "  pro-democracy/JJ\n",
        "  demonstrations/NNS\n",
        "  in/IN\n",
        "  (GPE Hong/NNP Kong/NNP)\n",
        "  ,/,\n",
        "  saying/VBG\n",
        "  that/IN\n",
        "  while/IN\n",
        "  the/DT\n",
        "  (GPE United/NNP States/NNPS)\n",
        "  did/VBD\n",
        "  not/RB\n",
        "  necessarily/RB\n",
        "  agree/VB\n",
        "  with/IN\n",
        "  (GPE China/NNP)\n",
        "  about/IN\n",
        "  the/DT\n",
        "  dispute/NN\n",
        "  ,/,\n",
        "  it/PRP\n",
        "  did/VBD\n",
        "  not/RB\n",
        "  want/VB\n",
        "  to/TO\n",
        "  see/VB\n",
        "  the/DT\n",
        "  tensions/NNS\n",
        "  in/IN\n",
        "  the/DT\n",
        "  city/NN\n",
        "  erupt/NN\n",
        "  into/IN\n",
        "  violence/NN\n",
        "  ./.)\n",
        "(S\n",
        "  (PERSON Mr./NNP)\n",
        "  Obama\\u2019s/NNP\n",
        "  carefully/RB\n",
        "  calibrated/VBD\n",
        "  remarks/NNS\n",
        "  were/VBD\n",
        "  his/PRP$\n",
        "  first/JJ\n",
        "  on/IN\n",
        "  the/DT\n",
        "  (GPE Hong/NNP Kong/NNP)\n",
        "  protests/NNS\n",
        "  since/IN\n",
        "  arriving/VBG\n",
        "  in/IN\n",
        "  (GPE Beijing/NNP)\n",
        "  earlier/RBR\n",
        "  in/IN\n",
        "  the/DT\n",
        "  day/NN\n",
        "  ./.)\n",
        "(S\n",
        "  \\u201cObviously/RB\n",
        "  ,/,\n",
        "  the/DT\n",
        "  situation/NN\n",
        "  between/IN\n",
        "  (GPE China/NNP)\n",
        "  and/CC\n",
        "  (GPE Hong/NNP Kong/NNP)\n",
        "  is/VBZ\n",
        "  historically/RB\n",
        "  complicated/VBN\n",
        "  and/CC\n",
        "  is/VBZ\n",
        "  in/IN\n",
        "  the/DT\n",
        "  process/NN\n",
        "  of/IN\n",
        "  transition/NN\n",
        "  ,/,\n",
        "  \\u201d/:\n",
        "  (PERSON Mr./NNP Obama/NNP)\n",
        "  said/VBD\n",
        "  ./.)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(S\n",
        "  \\u201cOur/JJ\n",
        "  primary/JJ\n",
        "  message/NN\n",
        "  has/VBZ\n",
        "  been/VBN\n",
        "  to/TO\n",
        "  make/VB\n",
        "  sure/JJ\n",
        "  violence/NN\n",
        "  is/VBZ\n",
        "  avoided/VBN\n",
        "  ,/,\n",
        "  \\u201d/NNP\n",
        "  the/DT\n",
        "  president/NN\n",
        "  said/VBD\n",
        "  ,/,\n",
        "  adding/NN\n",
        "  ,/,\n",
        "  \\u201cWe/JJ\n",
        "  don\\u2019t/NN\n",
        "  expect/NN\n",
        "  (GPE China/NNP)\n",
        "  to/TO\n",
        "  follow/VB\n",
        "  an/DT\n",
        "  (GPE American/JJ)\n",
        "  model/NN\n",
        "  in/IN\n",
        "  every/DT\n",
        "  instance/NN\n",
        "  ./.)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(S\n",
        "  But/CC\n",
        "  we\\u2019re/JJ\n",
        "  going/NN\n",
        "  to/TO\n",
        "  continue/VB\n",
        "  to/TO\n",
        "  have/VB\n",
        "  concerns/NNS\n",
        "  about/IN\n",
        "  human/JJ\n",
        "  rights.\\u201d/NN)\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}